%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Tommaso Rigon at 2022-01-17 10:58:09 +0100 


%% Saved with string encoding Unicode (UTF-8) 


@comment{jabref-meta: databaseType:bibtex;}



@article{Sur2019,
	author = {Sur, P and Cand{\`e}s, E. J.},
	date-added = {2022-01-17 10:57:00 +0100},
	date-modified = {2022-01-17 10:58:06 +0100},
	journal = {Proc. Nat. Acad. Sci.},
	pages = {14516--25},
	title = {A modern maximum-likelihood theory for high-dimensional logistic regression},
	volume = {116},
	year = {2019}}

@article{Greenland2015,
	abstract = {Penalization is a very general method of stabilizing or regularizing estimates, which has both frequentist and Bayesian rationales. We consider some questions that arise when considering alternative penalties for logistic regression and related models. The most widely programmed penalty appears to be the Firth small-sample bias-reduction method (albeit with small differences among implementations and the results they provide), which corresponds to using the log density of the Jeffreys invariant prior distribution as a penalty function. The latter representation raises some serious contextual objections to the Firth reduction, which also apply to alternative penalties based on t-distributions (including Cauchy priors). Taking simplicity of implementation and interpretation as our chief criteria, we propose that the log-F(1,1) prior provides a better default penalty than other proposals. Penalization based on more general log-F priors is trivial to implement and facilitates mean-squared error reduction and sensitivity analyses of penalty strength by varying the number of prior degrees of freedom. We caution however against penalization of intercepts, which are unduly sensitive to covariate coding and design idiosyncrasies.},
	author = {Greenland, Sander and Mansournia, Mohammad Ali},
	date-added = {2022-01-14 12:26:20 +0100},
	date-modified = {2022-01-14 12:26:41 +0100},
	doi = {10.1002/sim.6537},
	file = {:Users/tommaso/Desktop/Statistics in Medicine - 2015 - Greenland - Penalization bias reduction and default priors in logistic and related.pdf:pdf},
	issn = {10970258},
	journal = {Statist. Med.},
	keywords = {Bayes estimators,Bias correction,Firth bias reduction,Jeffreys prior,Logistic regression,Maximum likelihood,Penalized likelihood,Regularization,Shrinkage,Sparse data,Stabilization},
	number = {23},
	pages = {3133--3143},
	pmid = {26011599},
	title = {{Penalization, bias reduction, and default priors in logistic and related categorical and survival regressions}},
	volume = {34},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1002/sim.6537}}

@article{Greenland2003,
	abstract = {Conjugate priors for Bayesian analyses of relative risks can be quite restrictive, because their shape depends on their location. By introducing a separate location parameter, however, these priors generalize to allow modeling of a broad range of prior opinions, while still preserving the computational simplicity of conjugate analyses. The present article illustrates the resulting generalized conjugate analyses using examples from case-control studies of the association of residential wire codes and magnetic fields with childhood leukemia.},
	author = {Greenland, Sander},
	date-added = {2022-01-14 12:22:06 +0100},
	date-modified = {2022-01-14 12:22:06 +0100},
	doi = {10.1111/1541-0420.00011},
	file = {:Users/tommaso/Desktop/Biometrics - 2003 - Greenland - Generalized Conjugate Priors for Bayesian Analysis of Risk and Survival Regressions.pdf:pdf},
	issn = {0006341X},
	journal = {Biometrics},
	keywords = {Bayesian inference,Case-control studies,Conjugate prior,Cox model,Epidemiologic methods,Logistic regression,Odds ratio,Poisson regression,Proportional-hazards regression,Relative risk,Risk analysis,Risk assessment,Risk ratio,Survival regression},
	number = {1},
	pages = {92--99},
	pmid = {12762445},
	title = {{Generalized conjugate priors for Bayesian analysis of risk and survival regressions}},
	volume = {59},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1111/1541-0420.00011}}

@article{Chopin2017,
	abstract = {Whenever a new approach to perform Bayesian computation is introduced, a common practice is to showcase this approach on a binary regression model and datasets of moderate size. This paper discusses to which extent this practice is sound. It also reviews the current state of the art of Bayesian computation, using binary regression as a running example. Both sampling-based algorithms (importance sampling, MCMC and SMC) and fast approximations (Laplace, VB and EP) are covered. Extensive numerical results are provided, and are used to make recommendations to both end users and Bayesian computation experts. Implications for other problems (variable selection) and other models are also discussed.},
	archiveprefix = {arXiv},
	arxivid = {1506.08640},
	author = {Chopin, Nicolas and Ridgway, James},
	date-added = {2022-01-10 22:36:30 +0100},
	date-modified = {2022-01-10 22:37:20 +0100},
	doi = {10.1214/16-STS581},
	eprint = {1506.08640},
	file = {:Users/tommaso/Library/Application Support/Mendeley Desktop/Downloaded/Chopin, Ridgway - 2017 - Leave pima Indians alone Binary regression as a benchmark for Bayesian computation.pdf:pdf},
	issn = {08834237},
	journal = {Statist. Sc.},
	keywords = {Bayesian computation,Expectation propagation,Markov chain Monte Carlo,Sequential Monte Carlo,Variational inference},
	number = {1},
	pages = {64--87},
	title = {{Leave pima Indians alone: Binary regression as a benchmark for Bayesian computation}},
	volume = {32},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1214/16-STS581}}

@article{Haberman1973,
	author = {Haberman, Shelby J.},
	date-added = {2022-01-10 22:27:08 +0100},
	date-modified = {2022-01-10 22:27:17 +0100},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Annals of Statistics/Haberman - 1973 - Log-linear models for frequency data sufficient statistics and likelihood equations.pdf:pdf},
	journal = {Ann. Statist.},
	number = {4},
	pages = {617--632},
	title = {{Log-linear models for frequency data: sufficient statistics and likelihood equations}},
	volume = {1},
	year = {1973}}

@article{Ghosh2018,
	abstract = {In logistic regression, separation occurs when a linear combination of the predictors can perfectly classify part or all of the observations in the sample, and as a result, finite maximum likelihood estimates of the regression coefficients do not exist. Gelman et al. (2008) recommended independent Cauchy distribu- tions as default priors for the regression coefficients in logistic regression, even in the case of separation, and reported posterior modes in their analyses. As the mean does not exist for the Cauchy prior, a natural question is whether the pos- terior means of the regression coefficients exist under separation. We prove theo- rems that provide necessary and sufficient conditions for the existence of posterior means under independent Cauchy priors for the logit link and a general family of link functions, including the probit link. We also study the existence of posterior means under multivariate Cauchy priors. For full Bayesian inference, we develop a Gibbs sampler based on P´olya-Gamma data augmentation to sample from the posterior distribution under independent Student-t priors including Cauchy priors, and provide a companion R package tglm, available at CRAN. We demonstrate empirically that even when the posterior means of the regression coefficients exist under separation, the magnitude of the posterior samples for Cauchy priors may be unusually large, and the corresponding Gibbs sampler shows extremely slow mixing. While alternative algorithms such as the No-U-Turn Sampler (NUTS) in Stan can greatly improve mixing, in order to resolve the issue of extremely heavy tailed posteriors for Cauchy priors under separation, one would need to consider lighter tailed priors such as normal priors or Student-t priors with degrees of freedom larger than one.},
	archiveprefix = {arXiv},
	arxivid = {arXiv:1507.07170v2},
	author = {Ghosh, Joyee and Li, Yingbo and Mitra, Robin},
	date-added = {2022-01-10 13:47:15 +0100},
	date-modified = {2022-01-15 15:19:31 +0100},
	eprint = {arXiv:1507.07170v2},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Unknown/Ghosh, Li, Mitra - 2018 - On the Use of Cauchy Prior Distributions.pdf:pdf},
	issn = {1936-0975},
	journal = {Bayesian Anal.},
	keywords = {Ma,binary regression,carlo,existence of posterior mean,markov chain monte,probit regression,separation,slow mixing},
	number = {2},
	pages = {359--383},
	title = {{On the use of Cauchy prior distributions}},
	volume = {13},
	year = {2018}}

@article{Gelman2008,
	abstract = {We propose a new prior distribution for classical (nonhierarchical) logistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-t prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors. We recommend this prior distribution as a default choice for routine applied use. It has the advantage of always giving answers, even when there is complete separation in logistic regression (a common problem, even when the sample size is large and the number of predictors is small), and also automatically applying more shrinkage to higher-order interactions. This can be useful in routine data analysis as well as in automated procedures such as chained equations for missing-data imputation. We implement a procedure to fit generalized linear models in R with the Student-t prior distribution by incorporating an approximate EM algorithm into the usual iteratively weighted least squares. We illustrate with several applications, including a series of logistic regressions predicting voting preferences, a small bioassay experiment, and an imputation model for a public health data set. {\textcopyright} Institute of Mathematical Statistics.},
	archiveprefix = {arXiv},
	arxivid = {0901.4011},
	author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu Sung},
	date-added = {2022-01-09 15:24:41 +0100},
	date-modified = {2022-01-15 09:24:57 +0100},
	doi = {10.1214/08-AOAS191},
	eprint = {0901.4011},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Annals of Applied Statistics/Gelman et al. - 2008 - A weakly informative default prior distribution for logistic and other regression models.pdf:pdf},
	issn = {19326157},
	journal = {Ann. Appl. Statist.},
	keywords = {Bayesian inference,Generalized linear model,Hierarchical model,Least squares,Linear regression,Logistic regression,Multilevel model,Noninformative prior distribution,Weakly informative prior distribution},
	number = {4},
	pages = {1360--1383},
	title = {{A weakly informative default prior distribution for logistic and other regression models}},
	volume = {2},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1214/08-AOAS191}}

@article{Robert1996,
	author = {Robert, Christian P.},
	date-added = {2022-01-09 15:04:53 +0100},
	date-modified = {2022-01-15 09:25:33 +0100},
	file = {:Users/tommaso/Library/Application Support/Mendeley Desktop/Downloaded/Robert - 1996 - Intrinsic losses.pdf:pdf},
	journal = {Th. Dec.},
	number = {2},
	pages = {191--214},
	title = {{Intrinsic losses}},
	volume = {40},
	year = {1996}}

@book{Wasserman2005,
	author = {Wasserman, Larry},
	booktitle = {Springer},
	date-added = {2022-01-08 19:09:54 +0100},
	date-modified = {2022-01-08 19:09:54 +0100},
	file = {:Users/tommaso/Library/Application Support/Mendeley Desktop/Downloaded/Wasserman - 2005 - All of nonparametric statistics.pdf:pdf},
	publisher = {Springer},
	title = {{All of nonparametric statistics}},
	year = {2005}}

@article{Haldane1955,
	author = {Haldane, J. S. B.},
	date-added = {2022-01-08 14:24:03 +0100},
	date-modified = {2022-01-08 14:25:05 +0100},
	journal = {Ann. Hum. Gen.},
	pages = {309--311},
	title = {The estimation and significane of the logarithm oa ration of frequencies},
	volume = {20},
	year = {1955}}

@article{Anscombe1956,
	author = {Anscombe, F. G.},
	date-added = {2022-01-08 14:21:47 +0100},
	date-modified = {2022-01-08 14:22:40 +0100},
	journal = {Biometrika},
	pages = {461--464},
	title = {On estimating binomial response relations},
	volume = {43},
	year = {1956}}

@article{Lunardon2018,
	abstract = {Firth (1993) introduced a method for reducing the bias of the maximum likelihood estimator. Here it is shown that the approach is also effective in reducing the sensitivity of inferential procedures to incidental parameters.},
	author = {Lunardon, N.},
	date-added = {2022-01-07 18:29:57 +0100},
	date-modified = {2022-01-07 18:29:57 +0100},
	doi = {10.1093/biomet/asx079},
	file = {:Users/tommaso/Desktop/asx079.pdf:pdf},
	issn = {14643510},
	journal = {Biometrika},
	keywords = {Bartlett identity,Binary matched pairs,Modified profile likelihood,Two-index asymptotics},
	number = {1},
	pages = {233--238},
	title = {{On bias reduction and incidental parameters}},
	volume = {105},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1093/biomet/asx079}}

@misc{Kosmidis2020b,
	author = {Kosmidis, Ioannis},
	date-added = {2022-01-07 18:19:15 +0100},
	date-modified = {2022-01-07 18:22:06 +0100},
	title = {{brglm2: Bias Reduction in Generalized Linear Models. R package version 0.6.2. https://CRAN.R- project.org/package=brglm2.}},
	urldate = {2020},
	year = {2020}}

@article{Gart1985,
	abstract = {The bias and first four cumulants of the empirical logit transformation of a binomial variate are studied by means of asymptotic expansions and exact computation. The covariance of the empirical logit and its estimated variance is derived. Neither the +2-Jan correction of Haldane (1955) and Anscombe (1956) nor the -2-Jan suggested by Cox (1970) for weighted logit regression is universally effective in reducing bias. Other corrections are considered. The distribution of the empirical logit is shown to be considerably more skewed and to have a much larger kurtosis than the comparable binomial distribution. Methods based directly on sufficient statistics are preferred to those based on the empirical logit. {\textcopyright} 1985 Biometrika Trust.},
	author = {Gart, John J. and Pettigrew, Hugh M. and Thomas, Donald G.},
	date-added = {2021-12-22 23:36:35 +0100},
	date-modified = {2021-12-22 23:36:35 +0100},
	doi = {10.1093/biomet/72.1.179},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Unknown/Thomas - 1985 - The effect of bias , variance estimation , skewness and kurtosis of the empirical logit on weighted least squares analys.pdf:pdf},
	issn = {00063444},
	journal = {Biometrika},
	keywords = {Bias,Binomial distribution,Confidence interval,Interaction in 2×2×2 tables,Kurtosis,Logistic model,Logit,Odds ratio estimation,Simple logistic regression,Skewness},
	number = {1},
	pages = {179--190},
	title = {{The effect of bias, variance estimation, skewness and kurtosis of the empirical logit on weighted least squares analyses}},
	volume = {72},
	year = {1985},
	bdsk-url-1 = {https://doi.org/10.1093/biomet/72.1.179}}

@article{Heinze2002,
	abstract = {The phenomenon of separation or monotone likelihood is observed in the fitting process of a logistic model if the likelihood converges while at least one parameter estimate diverges to $\pm$ infinity. Separation primarily occurs in small samples with several unbalanced and highly predictive risk factors. A procedure by Firth originally developed to reduce the bias of maximum likelihood estimates is shown to provide an ideal solution to separation. It produces finite parameter estimates by means of penalized maximum likelihood estimation. Corresponding Wald tests and confidence intervals are available but it is shown that penalized likelihood ratio tests and profile penalized likelihood confidence intervals are often preferable. The clear advantage of the procedure over previous options of analysis is impressively demonstrated by the statistical analysis of two cancer studies. Copyright {\textcopyright} 2002 John Wiley & Sons, Ltd.},
	author = {Heinze, Georg and Schemper, Michael},
	date-added = {2021-12-22 23:16:40 +0100},
	date-modified = {2021-12-22 23:17:01 +0100},
	doi = {10.1002/sim.1047},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Unknown/Heinze, Schemper - 2002 - A solution to the problem of separation in logistic regression.pdf:pdf},
	issn = {02776715},
	journal = {Statist. Med.},
	keywords = {Bias reduction,Case-control studies,Infinite estimates,Monotone likelihood,Penalized likelihood,Profile likelihood},
	number = {16},
	pages = {2409--2419},
	pmid = {12210625},
	title = {{A solution to the problem of separation in logistic regression}},
	volume = {21},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1002/sim.1047}}

@article{Albert1984,
	author = {Albert, A. and Anderson, J. A.},
	date-added = {2021-12-22 23:08:28 +0100},
	date-modified = {2021-12-22 23:08:28 +0100},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Biometrika/Albert, Anderson - 1984 - On the existence of maximum likelihood estimates in logistic regression models.pdf:pdf},
	journal = {Biometrika},
	number = {1},
	pages = {1--10},
	title = {{On the existence of maximum likelihood estimates in logistic regression models}},
	volume = {71},
	year = {1984}}

@article{Wedderburn1976,
	author = {Wedderburn, R. W. M.},
	date-added = {2021-12-22 22:59:57 +0100},
	date-modified = {2021-12-22 22:59:57 +0100},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Biometrika/Wedderburn - 1976 - On the existence and uniqueness of the maximum likelihood estimates for certain generalized linear models.pdf:pdf},
	journal = {Biometrika},
	number = {1},
	pages = {27--32},
	title = {{On the existence and uniqueness of the maximum likelihood estimates for certain generalized linear models}},
	volume = {63},
	year = {1976}}

@article{Green1984,
	author = {Green, Peter J.},
	date-added = {2021-12-22 22:52:51 +0100},
	date-modified = {2021-12-22 22:52:58 +0100},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Journal of the Royal Statistical Society. Series B Statistical Methodology/Green - 1984 - Iteratively reweighted least squares for maximum likelihood estimation and some robust and resistant alternatives.pdf:pdf},
	journal = {J. R. Statist. Soc. B},
	number = {2},
	pages = {149--192},
	title = {{Iteratively reweighted least squares for maximum likelihood estimation and some robust and resistant alternatives}},
	volume = {46},
	year = {1984}}

@article{Kosmidis2009,
	abstract = {In Firth (1993, Biometrika) it was shown how the leading term in the asymptotic bias of the maximum likelihood estimator is removed by adjusting the score vector, and that in canonical-link generalized linear models the method is equivalent to maximizing a penalized likelihood that is easily implemented via iterative adjustment of the data. Here a more general family of bias-reducing adjustments is developed for a broad class of univariate and multivariate generalized nonlinear models. The resulting formulae for the adjusted score vector are computationally convenient, and in univariate models they directly suggest implementation through an iterative scheme of data adjustment. For generalized linear models a necessary and sufficient condition is given for the existence of a penalized likelihood interpretation of the method. An illustrative application to the Goodman row-column association model shows how the computational simplicity and statistical benefits of bias reduction extend beyond generalized linear models. {\textcopyright} 2009 Biometrika Trust.},
	author = {Kosmidis, Ioannis and Firth, David},
	date-added = {2021-12-22 22:34:52 +0100},
	date-modified = {2021-12-22 22:34:52 +0100},
	doi = {10.1093/biomet/asp055},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Biometrika/Kosmidis, Firth - 2009 - Bias reduction in exponential family nonlinear models.pdf:pdf},
	issn = {00063444},
	journal = {Biometrika},
	keywords = {Asymptotic bias correction,Generalized nonlinear model,Multivariate generalized linear model,Penalized likelihood,Pseudo-data},
	number = {4},
	pages = {793--804},
	title = {{Bias reduction in exponential family nonlinear models}},
	volume = {96},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1093/biomet/asp055}}

@article{Kosmidis2010,
	abstract = {A general iterative algorithm is developed for the computation of reduced-bias parameter estimates in regular statistical models through adjustments to the score function. The algorithm unifies and provides appealing new interpretation for iterative methods that have been published previously for some specific model classes. The new algorithm can use-fully be viewed as a series of iterative bias corrections, thus facilitating the adjusted score approach to bias reduction in any model for which the first-order bias of the maximum likelihood estimator has already been derived. The method is tested by application to a logit-linear multiple regression model with beta-distributed responses; the results confirm the effectiveness of the new algorithm, and also reveal some important errors in the existing literature on beta regression. {\textcopyright} 2010, Institute of Mathematical Statistics. All rights reserved.},
	author = {Kosmidis, Ioannis and Firth, David},
	date-added = {2021-12-22 22:29:07 +0100},
	date-modified = {2021-12-22 22:48:23 +0100},
	doi = {10.1214/10-EJS579},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Unknown/Kosmidis - 2010 - A generic algorithm for reducing bias in parametric estimation.pdf:pdf},
	issn = {19357524},
	journal = {Electr. J. Statist.},
	keywords = {Adjusted score,Asymptotic bias correction,Beta regression,Bias reduction,Fisher scoring,Prater gasoline data},
	pages = {1097--1112},
	title = {{A generic algorithm for reducing bias in parametric estimation}},
	volume = {4},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1214/10-EJS579}}

@article{Kosmidis2021,
	abstract = {Penalization of the likelihood by Jeffreys' invariant prior, or a positive power thereof, is shown to produce finite-valued maximum penalized likelihood estimates in a broad class of binomial generalized linear models. The class of models includes logistic regression, where the Jeffreys-prior penalty is known additionally to reduce the asymptotic bias of the maximum likelihood estimator, and models with other commonly used link functions, such as probit and log-log. Shrinkage towards equiprobability across observations, relative to the maximum likelihood estimator, is established theoretically and studied through illustrative examples. Some implications of finiteness and shrinkage for inference are discussed, particularly when inference is based on Wald-type procedures. A widely applicable procedure is developed for computation of maximum penalized likelihood estimates, by using repeated maximum likelihood fits with iteratively adjusted binomial responses and totals. These theoretical results and methods underpin the increasingly widespread use of reduced-bias and similarly penalized binomial regression models in many applied fields.},
	archiveprefix = {arXiv},
	arxivid = {1812.01938},
	author = {Kosmidis, Ioannis and Firth, David},
	date-added = {2021-12-22 22:27:31 +0100},
	date-modified = {2021-12-22 22:27:31 +0100},
	doi = {10.1093/biomet/asaa052},
	eprint = {1812.01938},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Biometrika/Kosmidis, Firth - 2021 - Jeffreys-prior penalty, finiteness and shrinkage in binomial-response generalized linear models.pdf:pdf},
	issn = {14643510},
	journal = {Biometrika},
	keywords = {Bias reduction,Bradley-Terry model,Data separation,Infinite estimate,Logit link,Penalized likelihood,Probit link,Working weight},
	number = {1},
	pages = {71--82},
	title = {{Jeffreys-prior penalty, finiteness and shrinkage in binomial-response generalized linear models}},
	volume = {108},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1093/biomet/asaa052}}

@article{Kosmidis2020,
	abstract = {This paper presents an integrated framework for estimation and inference from generalized linear models using adjusted score equations that result in mean and median bias reduction. The framework unifies theoretical and methodological aspects of past research on mean bias reduction and accommodates, in a natural way, new advances on median bias reduction. General expressions for the adjusted score functions are derived in terms of quantities that are readily available in standard software for fitting generalized linear models. The resulting estimating equations are solved using a unifying quasi-Fisher scoring algorithm that is shown to be equivalent to iteratively reweighted least squares with appropriately adjusted working variates. Formal links between the iterations for mean and median bias reduction are established. Core model invariance properties are used to develop a novel mixed adjustment strategy when the estimation of a dispersion parameter is necessary. It is also shown how median bias reduction in multinomial logistic regression can be done using the equivalent Poisson log-linear model. The estimates coming out from mean and median bias reduction are found to overcome practical issues related to infinite estimates that can occur with positive probability in generalized linear models with multinomial or discrete responses, and can result in valid inferences even in the presence of a high-dimensional nuisance parameter.},
	archiveprefix = {arXiv},
	arxivid = {1804.04085},
	author = {Kosmidis, Ioannis and {Kenne Pagui}, Euloge Clovis and Sartori, Nicola},
	date-added = {2021-12-22 22:26:40 +0100},
	date-modified = {2021-12-22 22:48:49 +0100},
	doi = {10.1007/s11222-019-09860-6},
	eprint = {1804.04085},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Statistics and Computing/Kosmidis, Kenne Pagui, Sartori - 2020 - Mean and median bias reduction in generalized linear models.pdf:pdf},
	issn = {15731375},
	journal = {Statist. Comp.},
	keywords = {Adjusted score equations,Data separation,Dispersion,Iterative reweighted least squares,Multinomial regression,Parameterization invariance},
	number = {1},
	pages = {43--59},
	publisher = {Springer US},
	title = {{Mean and median bias reduction in generalized linear models}},
	url = {https://doi.org/10.1007/s11222-019-09860-6},
	volume = {30},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/s11222-019-09860-6}}

@article{Cordeiro1991,
	author = {Cordeiro, Gauss M. and McCullagh, P.},
	date-added = {2021-12-22 22:26:24 +0100},
	date-modified = {2021-12-22 22:47:31 +0100},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Journal of the Royal Statistical Society. Series B Statistical Methodology/Cordeiro, McCullagh - 1991 - Bias Correction in Generalized Linear Models.pdf:pdf},
	journal = {J. R. Statist. Soc. B},
	keywords = {bias correction,canonical link,deviance,dispersion parameter},
	number = {3},
	pages = {629--643},
	title = {{Bias Correction in Generalized Linear Models}},
	volume = {53},
	year = {1991}}

@article{Pagui2017,
	author = {{Kenne Pagui}, E. C. and Salvan, A. and Sartori, N.},
	date-added = {2021-12-22 22:25:34 +0100},
	date-modified = {2021-12-22 22:25:51 +0100},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Biometrika/Kenne Pagui, Salvan, Sartori - 2017 - Median bias reduction of maximum likelihood estimates.pdf:pdf},
	journal = {Biometrika},
	number = {4},
	pages = {923--938},
	title = {{Median bias reduction of maximum likelihood estimates}},
	volume = {104},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1093/biomet/asx046}}

@article{Clogg1991,
	abstract = {We describe methods used to create a new Census data base that can be used to study comparability of industry and occupation classification systems. This project represents the most extensive application of multiple imputation to date, and the modeling effort was considerable as well---hundreds of logistic regressions were estimated. One goal of this article is to summarize the strategies used in the project so that researchers can better understand how the new data bases were created. Another goal is to show how modifications of maximum likelihood methods were made for the modeling and imputation phases of the project. To multiply-impute 1980 census-comparable codes for industries and occupations in two 1970 census public-use samples, logistic regression models were estimated with flattening constants. For many of the regression models considered, the data were too sparse to support conventional maximum likelihood analysis, so some alternative had to be employed. These methods solve existence and related computational problems often encountered with maximum likelihood methods. Inferences pertaining to effects of predictor variables and inferences regarding predictions from logit models are also more satisfactory. The Bayesian strategy used in this project can be applied in other sparse-data settings where logistic regression is used because the approach can be implemented easily with any standard computer program for logit regression or log-linear analysis. {\textcopyright} 1991 Taylor & Francis Group, LLC.},
	author = {Clogg, Clifford C. and Rubin, Donald B. and Schenker, Nathaniel and Schultz, Bradley and Weidman, Lynn},
	date-added = {2021-12-22 22:24:10 +0100},
	date-modified = {2021-12-22 22:47:22 +0100},
	doi = {10.1080/01621459.1991.10475005},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Unknown/Clogg et al. - 1991 - Multiple Imputation of Industry and Occupation Codes in Census Public-Use Samples Using Bayesian Logistic Regressi.pdf:pdf},
	issn = {1537274X},
	journal = {J. Am. Statist. Assoc.},
	keywords = {Maximum likelihood,Missing data,Multiple imputation,Prior distribution,Sparse data,Structural zeros},
	number = {413},
	pages = {68--78},
	title = {{Multiple imputation of industry and occupation codes in census public-use samples using Bayesian logistic regression}},
	volume = {86},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.1991.10475005}}

@article{Durante2019,
	abstract = {Variational Bayes (VB) is a common strategy for approximate Bayesian inference, but simple methods are only available for specific classes of models including, in particular, representations having conditionally conjugate constructions within an exponential family. Models with logit components are an apparently notable exception to this class, due to the absence of conjugacy among the logistic likelihood and the Gaussian priors for the coefficients in the linear predictor. To facilitate approximate inference within this widely used class of models, Jaakkola and Jordan (Stat. Comput. 10 (2000) 25-37) proposed a simple variational approach which relies on a family of tangent quadratic lower bounds of the logistic log-likelihood, thus restoring conjugacy between these approximate bounds and the Gaussian priors. This strategy is still implemented successfully, but few attempts have been made to formally understand the reasons underlying its excellent performance. Following a review on VB for logistic models, we cover this gap by providing a formal connection between the above bound and a recent P{\'{o}}lya-gamma data augmentation for logistic regression. Such a result places the computational methods associated with the aforementioned bounds within the framework of variational inference for conditionally conjugate exponential family models, thereby allowing recent advances for this class to be inherited also by the methods relying on Jaakkola and Jordan (Stat. Comput. 10 (2000) 25-37).},
	author = {Durante, D. and Rigon, T.},
	date-added = {2021-12-22 14:30:20 +0100},
	date-modified = {2021-12-22 22:53:39 +0100},
	journal = {Statist. Sc.},
	number = {3},
	pages = {472--485},
	title = {{Conditionally conjugate mean-field variational Bayes for logistic models}},
	volume = {34},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1214/19-STS712}}

@article{Polson2013,
	author = {Polson, Nicholas G. and Scott, James G. and Windle, Jesse},
	date-added = {2021-12-22 11:07:00 +0100},
	date-modified = {2021-12-22 22:48:59 +0100},
	file = {:Users/tommaso/Library/Application Support/Mendeley Desktop/Downloaded/Polson, Scott, Windle - 2013 - Bayesian inference for logistic models using Polya-Gamma latent variables.pdf:pdf},
	journal = {J. Am. Statist. Assoc.},
	number = {504},
	pages = {1--42},
	publisher = {Taylor {&} Francis},
	title = {{Bayesian inference for logistic models using Polya-Gamma latent variables}},
	volume = {108},
	year = {2013}}

@article{Chen2003,
	abstract = {We propose a novel class of conjugate priors for the family of generalized linear models. Properties of the priors are investigated in detail and elicitation issues are examined. We establish theorems characterizing the propriety and existence of moments of the priors under various settings, examine asymptotic properties of the priors, and investigate the relationship to normal priors. Our approach is based on the notion of specifying a prior prediction y0 for the response vector of the current study, and a scalar precision parameter a0 which quantifies one's prior belief in y0. Then (y0, a0), along with the covariate matrix X of the current study, are used to specify the conjugate prior for the regression coefficients $\beta$ in a generalized linear model. We examine properties of the prior for a0 fixed and for a0 random, and study elicitation strategies for (y0,a0) in detail. We also study generalized linear models with an unknown dispersion parameter. An example is given to demonstrate the properties of the prior and the resulting posterior.},
	author = {Chen, Ming Hui and Ibrahim, Joseph G.},
	date-added = {2021-12-21 14:13:04 +0100},
	date-modified = {2021-12-22 22:47:13 +0100},
	doi = {10.17615/x1km-ha64},
	file = {:Users/tommaso/Google Drive Bicocca/Books_and_papers/Statistica Sinica/Chen, Ibrahim - 2003 - Conjugate priors for generalized linear models.pdf:pdf},
	issn = {10170405},
	journal = {Statist. Sin.},
	keywords = {Conjugate prior,Generalized linear models,Gibbs sampling,Historical data,Logistic regression,Poisson regression,Predictive elicitation},
	number = {2},
	pages = {461--476},
	title = {{Conjugate priors for generalized linear models}},
	volume = {13},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.17615/x1km-ha64}}

@article{Diaconis1979,
	author = {Diaconis, Persi and Ylvisaker, Donald},
	date-added = {2021-12-21 12:25:43 +0100},
	date-modified = {2021-12-22 22:47:38 +0100},
	file = {:Users/tommaso/Library/Application Support/Mendeley Desktop/Downloaded/Diaconis, Ylvisaker - 1979 - Conjugate prior for exponential families.pdf:pdf},
	journal = {Ann. Statist.},
	number = {2},
	pages = {269--292},
	title = {{Conjugate prior for exponential families}},
	volume = {7},
	year = {1979}}

@book{McCullagh1989,
	author = {McCullagh, P. and Nelder, J. A.},
	booktitle = {Springer},
	date-added = {2021-12-21 11:54:09 +0100},
	date-modified = {2021-12-21 15:27:38 +0100},
	edition = {Second},
	file = {:Users/tommaso/Library/Application Support/Mendeley Desktop/Downloaded/McCullagh, Nelder - 1986 - Generalized Linear Models.pdf:pdf},
	publisher = {Springer},
	title = {{Generalized linear models}},
	year = {1989},
	bdsk-url-1 = {http://epubs.siam.org/doi/abs/10.1137/1028043}}
@book{Nocedal2006,
title="Conjugate Gradient Methods",
bookTitle="Numerical Optimization",
year="2006",
publisher="Springer New York",
address="New York, NY",
author = {Jorge Nocedal and Stephen Wright},
}

@book{Agresti2002,
	author = {Agresti, Alan},
	publisher = {Wiley},
	title = {Categorical Data Analysis},
	year = {2002}}

@article{Firth1993,
	author = {Firth, David},
	journal = {Biometrika},
	number = {1},
	pages = {27--38},
	title = {{Bias reduction of maximum likelihood estimates}},
	volume = {80},
	year = {1993}}

@article{Nelder1972,
	author = {Nelder, J. A. and Wedderburn, R. W. M.},
	date-modified = {2021-12-22 22:49:09 +0100},
	journal = {J. R. Statist. Soc. A},
	number = {3},
	pages = {370--384},
	title = {{Generalized linear models}},
	volume = {135},
	year = {1972}}

@book{Pace1997,
	author = {Pace, Luigi and Salvan, Alessandra},
	date-added = {2020-02-04 13:10:16 -0500},
	date-modified = {2020-02-04 13:11:40 -0500},
	publisher = {World Scientific},
	title = {Principles of Statistical Inference from a Neo-Fisherian Perspective},
	year = {1997}}
