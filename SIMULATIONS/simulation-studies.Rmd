---
title: "high-dimensional"
author: "Tommaso Rigon and Emanuele Aliverti"
output: html_document
bibliography: ["../biblio.bib"]
---

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
```

# Simulation studies

In this tutorial we focus on different simulation studies to evaluate
the performance of the proposed DY correction

## First example
The first example is taken from @Sur2019 and commented in @Kosmidis2021

```{r,echo=T}
set.seed(1)
n <- 1000
p <- 200
beta <- c(rep(10, p / 8), rep(-10, p / 8), rep(0, 3 * p / 4))
X <- matrix(rnorm(n * p, 0, sqrt(1 / n)), n, p)
y <- rbinom(n, 1, plogis(X %*% beta))
fit_mle <- glm(y ~ X - 1, family = binomial("logit"))
```



We estimate different corrections and compare their empirical performance in terms of
timing. Recalling that penalized likelihood optimization under the Diaconis-Ylvisaker conjugate prior 
induces a binomial likelihood with pseudo-counts, we can rely on different available optimization
 methods for logistic regression models. 
We found that quasi-Newton methods have better numerical performance; refer to @Nocedal2006 for
additional details.
A practical implementation for logistic regression optimization via L-BFGS is provided
in the function `fastLR` available in the package `RcppNumerical`


```{r run,echo=T,warning=F, eval=T}
library(brglm2)
m <- nrow(X)
y_dy <- p / (p + m) * 0.5 + m / (p + m) * y




library(RcppNumerical)
t0 <- Sys.time()
fit_fast_dy <- fastLR(X, y_dy)
t1 <- Sys.time()
elapsed_fast_dy <- t1 - t0
y_clogg <- p / (p + m) * mean(y) + m / (p + m) * y

# DY ESTIMATE
fit_fast_clogg <- fastLR(X, y_clogg)

# FIRTH (1993)
t0 <- Sys.time()
fit_firth <- glm(y ~ -1 + X,
  family = binomial("logit"),
  method = "brglmFit", type = "AS_mean"
)
t1 <- Sys.time()
elapsed_firth <- t1 - t0
```

```{r print-r,echo=F,eval=F}
tt = c(elapsed_dy, elapsed_fast_dy, elapsed_firth)
names(tt) = c("DY", "DY - fast implementation", "@Firth1993")
kable(tt,digits = 3,format = "markdown")
```
```{r, echo=F,warning=F,fig.height=3,fig.width=12,fig.align='center'}
cc = list("MLE" = coef(fit_mle), "Firth" = coef(fit_firth), 
          "Clogg" = fit_fast_clogg$coefficients,
          "DY" = fit_fast_dy$coefficients)
df_list = reshape2::melt(cc)
df_list$x=rep(1:p, length(cc))
df_list$b0 = rep(beta,length(cc))

df_list$L1 = ordered(factor(df_list$L1,levels = c("MLE", "DY", "Clogg", "Firth"), labels = 
         c("MLE","DY",  "Clogg (1991)", "Firth (1993)")))

ggplot(df_list) + 
  geom_point(aes(x,value)) + 
  geom_line(aes(x,b0), color = "red") + 
  facet_wrap(~L1,nrow = 1) +
  theme_bw() + xlab("") + ylab("")
```


We evaluate bias and rmse across 1000 replications of this scenario.
Computations takes roughly 1 hour on a 2020 Macbook Pro with M1 processor 
(`aarch64-apple-darwin20`)  running R 4.1.1 linked with `openblas`.
Results are stored in the file `n1000_p200_1.RData` and can be reproduced running
the script [`sur-candes-1.R`](https://raw.githubusercontent.com/tommasorigon/logistic-bias-reduction/main/SIMULATIONS/sur-candes-1.R)

```{r, echo=F,warning=F,fig.height=4,fig.width=6,fig.align='center'}
load("n1000_p200_1.RData")
df = reshape2::melt(list("Bias" = bias.beta, "RMSE" = sqrt(mse.beta)))
df$variable = factor(df$variable, levels = c("ml", "dy", "clogg", "br"),
 labels=                     c("MLE","DY",  "Clogg (1991)", "Firth (1993)"),ord=T)
library(ggplot2)
pl = ggplot(df) +
  geom_boxplot(aes(y = value, x = variable,  group = variable), color = "gray50") +
  geom_jitter(aes(y = value, x = variable,  group = variable), alpha = .2, color = "gray") +
  facet_wrap(~L1, scales = "free") +
  theme_bw() + xlab("") + ylab("") + 
  theme(legend.position = "none",  axis.text.x = element_text(angle = 60,hjust = 1)
        )
pl
```

# References