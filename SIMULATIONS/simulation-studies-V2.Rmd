---
title: "high-dimensional"
author: "Tommaso Rigon and Emanuele Aliverti"
output: html_document
bibliography: ["../biblio.bib"]
---

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
```

# Simulation studies

In this tutorial we focus on different simulation studies to evaluate
the performance of the proposed DY correction against competitors.

## First example
The first example is taken from Appendix D @Sur2019 and the Supplementary Materialds of @Kosmidis2021,
and focuses on a setting with $n=1000$ observations from a logisitc regression model with
$p=200$, $25%$ of coefficients equal to $10$, $25%$ of coefficients equal to $-10$ and the remaining set 
as $0$.

```{r,echo=T}
set.seed(1991)
n <- 1000
p <- 200
b0 = c(3,1.5,0, -1.5,-3)
beta = rep(b0,each = p/5)
X <- matrix(rnorm(n * (p), 0, sqrt(1 / n)), n, p)
y <- rbinom(n, 1, plogis(X %*% beta))
mean(y)
fit_mle <- glm(y ~ X - 1, family = binomial("logit"))
```



We estimate different corrections and compare their empirical performance in terms of
timing. Recalling that penalized likelihood optimization under the Diaconis-Ylvisaker conjugate prior 
induces a binomial likelihood with pseudo-counts, we can rely on different available optimization
 methods for logistic regression models. 
We found that quasi-Newton methods have better numerical performance; refer to @Nocedal2006 for
additional details.
A practical implementation for logistic regression optimization via L-BFGS is provided
in the function `fastLR` available in the package `RcppNumerical`


```{r run,echo=T,warning=F, eval=T}
library(brglm2)
m <- nrow(X)
y_dy <- p / (p + m) * 0.5 + m / (p + m) * y




library(RcppNumerical)
t0 <- Sys.time()
fit_fast_dy <- fastLR(X, y_dy)
t1 <- Sys.time()
elapsed_fast_dy <- t1 - t0
y_clogg <- p / (p + m) * mean(y) + m / (p + m) * y

# DY ESTIMATE
fit_fast_clogg <- fastLR(X, y_clogg)

# FIRTH (1993)
t0 <- Sys.time()
fit_firth <- glm(y ~ -1 + X,
  family = binomial("logit"),
  method = "brglmFit", type = "AS_mean"
)
t1 <- Sys.time()
elapsed_firth <- t1 - t0
```


```{r print-r,echo=F,eval=F}
tt = c(elapsed_dy, elapsed_fast_dy, elapsed_firth)
names(tt) = c("DY", "DY - fast implementation", "@Firth1993")
kable(tt,digits = 3,format = "markdown")
```

```{r timing, echo=F, eval = F}
library(microbenchmark)
microbenchmark(fastLR(X, y_dy), glm(y ~ -1 + X, family = binomial("logit"), method = "brglmFit", type = "AS_mean"),unit = "s")
```

```{r, echo=F,warning=F,fig.height=3,fig.width=12,fig.align='center'}
cc = list("MLE" = coef(fit_mle), "Firth" = coef(fit_firth), 
          "Clogg" = fit_fast_clogg$coefficients,
          "DY" = fit_fast_dy$coefficients)
df_list = reshape2::melt(cc)
df_list$x=rep(1:p, length(cc))
df_list$b0 = rep(beta,length(cc))

df_list$L1 = ordered(factor(df_list$L1,levels = c("MLE", "DY", "Clogg", "Firth"), labels = 
         c("Maximum-Likelihood","Diaconis-Ylvisaker",  "Clogg (1991)", "Firth (1993)")))
df_seg = data.frame(matrix(c(0,p/8,p/8+1,p/4,p/4+1,p),ncol=2,byrow = T),y=c(10,-10,0))
pl = ggplot(df_list) + 
  geom_point(aes(x,value), color = "gray50") + 
  geom_segment(data=df_seg, aes(x= X1, xend = X2, y = y, yend=y),size = 1.4, color = "black")+
  facet_wrap(~L1,nrow = 1) + 
  theme_bw() + xlab("") + ylab("Estimated coefficients")
pl
# ggsave(pl, file = "figs/coef.eps", width = 12,height = 3.1)
# ggsave(pl, file = "figs/coef.png", width = 12,height = 3)
# include_graphics("figs/coef.png")
```


We evaluate bias and RMSE across 5000 replications of this scenario.
Computations takes roughly 5 hours on a 2020 Macbook Pro with M1 processor 
(`aarch64-apple-darwin20`)  running R 4.1.1 linked with `openblas`.
Results are stored in the file `sur-candes.RData` and can be reproduced running
the script [`sur-candes.R`](https://raw.githubusercontent.com/tommasorigon/logistic-bias-reduction/main/SIMULATIONS/sur-candes.R)

```{r, echo=F,warning=F,message=FALSE,fig.height=3,fig.width=12,fig.align='center'}
load("sur-candes-v2.RData")
rmse.beta = sqrt(mse.beta)
bias.beta$x = 1:NROW(bias.beta)
rmse.beta$x = 1:NROW(rmse.beta)
# mae.beta$x = 1:NROW(mae.beta)
df = reshape2::melt(list("Bias" = bias.beta, "RMSE" = rmse.beta), id.var = "x")
df = subset(df, variable != "clogg")
df$variable = factor(df$variable, levels = c("ml", "dy",  "br"),
                     labels= c("Maximum\nlikelihood", "Diaconis\nYlvisaker",  "Firth (1993)"),ord=T)

df$xl = cut(df$x,breaks = seq(0,p,by=40), labels = paste0("beta==",c('3','3/2','0','-3/2','-3')))
line_df = subset(df, L1 == "Bias")
line_df = do.call(rbind,by(line_df,line_df$xl, head,1))
line_df$value = 0
library(ggplot2)
pl = ggplot(df) +
  geom_boxplot(aes(y = value, x = variable,  group = variable), color = "gray50") +
  # geom_jitter(aes(y = value, x = variable,  group = variable), alpha = .2, color = "gray") +
  # facet_wrap(~L1 + xl, scales = "free") +
   geom_hline(data = line_df, aes(yintercept=0), col = "gray50", lty = 3)+
  facet_grid(rows=vars(L1), cols = vars(xl), scales = "free",labeller = label_parsed) +
  theme_bw(base_size = 14) + xlab("") + ylab("") + 
  scale_fill_manual(values = c("black", "grey25", "grey75", "white"))+
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 60,hjust = 1,vjust = 1),
        strip.text = element_text(size=16)
        )
pl_min = pl + theme_minimal() + 
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 60,hjust = 1,vjust = 1),
        strip.text = element_text(size=16)
        ) 
# ggsave(pl, file = "figs/boxplot.pdf",  width = 11,height = 5)
# ggsave(pl, file = "figs/boxplot.eps",  width = 11,height = 5)
# ggsave(pl_min, file = "figs/boxplot_min.eps",  width = 11,height = 5)
# ggsave(pl, file = "figs/boxpl-1.png",  width = 12,height = 3.5)
include_graphics("figs/boxpl-1.png")
```


# References