---
title: "high-dimensional"
author: "Tommaso Rigon and Emanuele Aliverti"
output: html_document
bibliography: ["../biblio.bib"]
---

```{r setup, include=FALSE}
library(knitr)
```

# Example 1

This tutorial is devoted to comparing the computational performance of the
different corrections on a set of artificial data.
We generate simulated data from a logistic regression model, with correlated covariates and 
regression coefficient sampled from a uniform distribution.

```{r setup1}
set.seed(1991)
n <- 500
p <- 100

SS <- matrix(0.2, p - 1, p - 1)
diag(SS) <- 1

X <- cbind(1, matrix(rnorm(n * (p - 1)), n, p - 1) %*% chol(SS))
betas <- runif(p)
y <- rbinom(n, 1, prob = plogis(X %*% betas))
```

We estimate different corrections and compare their empirical performance in terms of
timing. Recalling that penalized likelihood optimization under the Diaconis-Ylvisaker conjugate prior 
induces a binomial likelihood with pseudo-counts, we can rely on different available optimization
 methods for logistic regression models. 
We found that quasi-Newton methods have better numerical performance; refer to @Nocedal2006 for
additional details.
A practical implementation for logistic regression optimization via L-BFGS is provided
in the function `fastLR` available in the package `RcppNumerical`


```{r run,echo=T,warning=F}
library(brglm2)
m <- nrow(X)
y_dy <- p / (p + m) * 0.5 + m / (p + m) * y

# DY ESTIMATE
t0 <- Sys.time()
fit_dy <- glm(y_dy ~ X - 1, family = binomial("logit"))
t1 <- Sys.time()
elapsed_dy <- t1 - t0

library(RcppNumerical)
t0 <- Sys.time()
fit_fast_dy <- fastLR(X, y_dy)
t1 <- Sys.time()
elapsed_fast_dy <- t1 - t0

# FIRTH (1993)
t0 <- Sys.time()
fit_firth <- glm(y ~ -1 + X,
  family = binomial("logit"),
  method = "brglmFit", type = "AS_mean"
)
t1 <- Sys.time()
elapsed_firth <- t1 - t0

# KENNE PAGUI ET AL. (2017)
t0 <- Sys.time()
fit_kp <- glm(y ~ -1 + X,
  family = binomial("logit"),
  method = "brglmFit", type = "AS_median"
)
t1 <- Sys.time()
elapsed_kp <- t1 - t0
```
Timing comparison are provided in the following table.
```{r print-r,echo=F}
tt = c(elapsed_dy, elapsed_fast_dy, elapsed_firth, elapsed_kp)
names(tt) = c("DY", "DY - fast implementation", "@Firth1993", "@Pagui2017")
kable(tt,digits = 3,format = "markdown")
```
Computational costs notably increases with $n$ and $p$, keeping the same ratio $p/n=0.2$.
For examples, with $n=5000$ and $p=1000$


```{r,eval=T,echo=F,warning=F}
set.seed(1)
n <- 5000
p <- 1000

SS <- matrix(0.2, p - 1, p - 1)
diag(SS) <- 1

X <- cbind(1, matrix(rnorm(n * (p - 1)), n, p - 1) %*% chol(SS))
betas <- runif(p)
y <- rbinom(n, 1, prob = plogis(X %*% betas))

library(brglm2)
m <- nrow(X)
y_dy <- p / (p + m) * 0.5 + m / (p + m) * y

# DY ESTIMATE
t0 <- Sys.time()
fit_dy <- glm(y_dy ~ X - 1, family = binomial("logit"))
t1 <- Sys.time()
elapsed_dy <- t1 - t0

library(RcppNumerical)
t0 <- Sys.time()
fit_fast_dy <- fastLR(X, y_dy)
t1 <- Sys.time()
elapsed_fast_dy <- t1 - t0

# FIRTH (1993)
t0 <- Sys.time()
fit_firth <- glm(y ~ -1 + X,
  family = binomial("logit"),
  method = "brglmFit", type = "AS_mean"
)
t1 <- Sys.time()
beta_firth <- coef(fit_firth)

# DO NOT EXECUTE

# KENNE PAGUI ET AL. (2017)
t0 <- Sys.time()
fit_kp <- glm(y ~ -1 + X,
  family = binomial("logit"),
  method = "brglmFit", type = "AS_median"
)
t1 <- Sys.time()
```

```{r print-r2,echo=F}
tt = c(elapsed_dy, elapsed_fast_dy, elapsed_firth, elapsed_kp)
names(tt) = c("DY", "DY - fast implementation", "@Firth1993", "@Pagui2017")
kable(tt,digits = 3,format = "markdown")
```
# References